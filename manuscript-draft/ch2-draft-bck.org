** Introduction
This chapter begins with a discussion on the dominant metaphors present in the modern DAW and expands on the earlier introduction to the topic. Some concrete examples are given to illuminate the issues and limitations that these metaphors impose on its users. This is followed up with a survey of legacy systems that take a graphical approach to their interface and can be conceptually framed with the metaphor of sketching.

** Dominant DAW metaphors
As has been asserted, analog studio metaphors of tape machines and hardware mixing desks dominate the UI approach to DAW interface design. Some other prevalent metaphors found is that of the /ou/, the /keyboard/ and the /score/ \cite{bell_journal_2015,levin_painterly_2000,adenot_web_2017,ableton_live_2017}. Clear examples of control panel metaphors can be found in plugin effects and present the user with banks of knobs and sliders that are often rendered as realistic imitations of their analog inspirations (figure fig:plugin-knobs-sliders).

*** Multi-track tape recorder
Multi track tape recording was introduced into the recording studios in the 1960s and is typified by the Ampeg???? and the Studer???? which allowed producers to record multiple tracks of audio information which could be edited and mixed to taste. This unlocked significant creative possibilities and made albums like "Sgt Peppers Lonely Heart Club Band" possible. The underlying model of tracks typically manifests itself in DAWs as rectangular blocks stacked from top to bottom and running from left to right. Similar to editing tape, these can be spliced, cut, and pasted. Terms and techniques like bouncing, overdubbing and markers that exist in DAWs all have their roots in their analog precedents.

*** Multi channel mixing desk
The multi channel mixing desk metaphor is present in the large majority of DAWs and is normally represented in a similar fashion to the sliders (or faders) found in hardware mixing desks (fig:mixing-desk). The mixing desk enabled the producer to control the relative amplitude of a finite amount of channels in addition to performing tasks such as panning to balance the signal between the two speakers. The slim vertical sliders found on most systems were codified in the 1960's by Bill Putnam in the 1960s (ref???) and were created with ergonomics in mind. This layout allowed the producer to manipulate multiple channels of audio simultaneosly in a practice known variously as "riding the faders" and "playing the mixer". Despite the fact that the digital variants of these are largely controlled by a mouse that only affords the effecting of a single fader at a time, they are still, largely speaking, presented in this fashion on screen. 

*** Outboard effects
The configuration and presentation of digital effects are another clear example of an analog metaphor that is omnipresent in DAWs. They are conventionally setup as insert effects which allows a single instrument to be affected and send effects which allows a certain amount of the signal of a track to be affected (see fig:outboard-fx). With the introduction of plugin instrument and effects by Steinberg in the form of their VST plugins, the visual look of these interfaces started to take on an ever increasing photo realistic imitation of their analog inspirations. Levin (???) quite aptly describes this phenomenon as skeumorphism, where objects that were inspired by a previous object add visual features of the original that are no longer needed in the new object. The purpose of this is both decorative to increase the emotional appeal of them and also educational in that it gives connotational cues to the user of how to use it.

*** (Score)

*** (Keyboard)

*** The piano roll
The piano roll is a primary metaphor found in almost all mainstream DAWS. This metaphor is one that is slightly distinct from the previously discussed examples in that it originates from a much earlier time period, the player pianos of the 1920s. It can also be seen as a combination of the keyboard and the score metaphor to lend it a sense of familiarity to even a novice user. The original piano rolls were operated by feeding a roll of paper with holes punched to indicate the precise timing that an attached piano should strike its notes. This provides an apt and suitable description for the /MIDI/ data it normally represents. MIDI, which stands for Musical Instrument Digital Interface is a standard protocol developed in the eighties to allow control instructions to be sent between devices. It provided a standard language to for instance tell a synthesizer to play a particular note for a certain duration of time. These instructions could be collected into a MIDI file to, in effect, create a playable digital score. Similar to a player piano, no audible results are possible without an attached piano or commonly in the case of MIDI a synthesizer device.

** A compositional example
Rather than discussing the issues that can arise from the metaphors in the abstract, let us consider a compositional idea and how we might acheive this in a DAW. The idea is broken into the following compositional "recipe": 
1. Two notes of the same timbre are played together about an octave apart for a duration of 2 seconds.
2. The first note glissandos to the frequency of the second note and vice versa.
3. The first note starts with a small amount of vibrato the quickly dissipates.
4. The second note starts with no vibrato but adds a small amount as the note nears completion.
5. When these two notes end, the same pattern is repeated except this time with different timbres and frequencies.
6. This is repeated 3 more times with different timbres and frequencies to make complete this ten-second piece.

While this may seem like a contrived example this, in fact, constitutes a compositional technique called Klangfarbenmelodie that involves splitting a melodic line between intruments or timbre to create a timbre melody. The glissandos and altering of vibrato intensity constitute add further complexity and better illustrate some of the weaknesses inherent in DAW metaphors.
 
*** Realization in a DAW
To achieve this in a DAW we have a few different options but a possible solution would be as follows:
1. Working with the multitrack tape metaphor we can create ten separate tracks to house two different versions of each timbre. Add a vibrato plugin effect to each of these by using a send or an insert effect. Two different tracks are needed for each of the timbres due to the fact that the two notes are played at the same time and both have different frequency and effect trajectories. If on the other hand they had the same effect modulations there would not need to be new tracks created.
2. Working with the piano roll metaphor, create a single note in each of these tracks setting each one to the desired fundamental frequency.
3. Now edit the pitch bend automation lane by clicking into the relevant dialog
3. Similarly, open the relevant dialog to edit the intensity of the vibrato effect 

At this point, we may have achieved what we set out to do. However, we now may want to tweak each of these elements to taste and perhaps add more material. An explosion in track count and overall complexity is inevitable. This can lead to a serious slowdown in workflow, a loss of flow and cognitive overload. A common technique to combat this complexity overload is to bounce the tracks and then continue working on these simpler artifacts (Duignan???). This, of course, negates a key advantage to working in a digital environment, the fine-grained ability to freely change, tweak and undo. There are of course other tools in the DAW that may achieve this task more easily. For instance, a sampler may allow us to use different timbres on the same track and may work better in this case. We now have the extra task of exporting each of these samples in preparation for our composition work. Some other options present in many DAWS include aggregate instruments, multi-timbral instruments, and perhaps some midi routing options. This points to another problem that is often encountered when using DAWs, that of option dilemma. Some alternative environments to realising the composition will now be considered.

*** Realisation in code
The piece could be realised in quite a straightforward manner in an audio programming language such as /csound/. Central to /csound/ is the concept of the /unit generator/, an abstraction to define both sound generators and processors. These can patched together in a simple textual coding language to form instruments. A score is then specified, again in code, to define note onsets, durations in addition to other arbitrary parameters defined in the instruments. Each of the required timbres could have been represented as separate csound instruments, with each one configured for with the desired timbre in addition to the vibrato effect. The score would refer to each of these timbres and use /function tables/ to define both the frequency trajectory and vibrato. Each note would be a single line of code, and making entire score a total of five lines. Demonstration code can be found in the appendix.

Depending on the experience of the reader, this may or may not seem like a better approach than using the DAW owing to a central issue to this approach. It is not beginner friendly and reasonable amount of prior experience and/or training. Perhaps a bigger criticism that could be made to this approach, however, is that it can lead to a more analytical rather than creative way of thinking. In "Thinking Slow, Acting Fast", Daniel Daniel Kahneman contrasts these two ways of thinking which he terms /System 1/ and /System 2/. System 1 is instinctive, fast, emotional and is a mode of thinking that may not register consciously. System 2 is slow, logical, analytical and registers prominently in active consciousness. Routine tasks such as walking, opening doors etc only use system 1 thinking. These can be completed without while exerting minimal cognitive effort to calculate the complex motor sensory actions that must take place to complete the task. Complex analytical tasks such as programming require system 2 thinking. Approaching creative tasks such as music making where instinct and emotion are often crucial can slow down or stop the process. Perhaps it is best summed by John Cage: "Don't try to create and analyse at the same time. They're different processes" (Popova, 2012).

** Sketching as an alternative metaphor
We shall now consider systems that use metaphors that are graphic nature. These are less common and if present less prominent in the DAW interface. As is pointed out by Levin (2002), the exploration of synchrony between audio and visuals is a practice going back centuries, being variously termed "ocular music, visual music, color music, or music for the eyes" (Levin, 2000). The twentieth-century technique of the optical soundtrack, however, brought these ideas to a new level of sophistication. The technique, which involved placing marks via photography or direct manipulation to specify audio properties, was explored by such luminaries as Oskar Fischinger, Norman McLaren and Daphne Oram. Oram's particular take on the technique will now be discussed as her system most closely resembles that of a piano roll.

*** (ANS Synth)

*** Oramics
A primary motivating factor behind Daphne Oram's development of the Oramics machine was to bring more human-like qualities to the sounds generated by electronic means. The machine worked by playing back multiple lanes of film tape in unison, defining a monophonic series of notes as well as control signals to shape their timbre, pitch and amplitude. She details the thought process behind this in her hugely insightful and broad ranging journal style book, "An Individual Note" (???).

The aspects of the sound that she wishes to control are volume, duration, timbre, pitch, vibrato and reverb. In order to do this she describes a simple musical notation language based on the freehand drawing of lines combined with discrete symbols. The lines, which she describes as the analog control, are used to define volume envelopes. Interestingly, the default and preferred method for the parameters she wishes to control is the continuous line rather than discrete note symbols. For instance, she avoids the use of a static velocity per note and instead only specifies the use of a control envelope to change amplitude.

The discrete symbols, which she categorizes as digital control, are used to define individual pitches and are termed neumes. She highlights that notes should not remain static and, thusly, an analog control of each note is also specified. Similarly to amplitude and vibrato, timbre is also defined by the freehand drawing of lines and is something that with practice the "inner ear" can develop an intuition as the sonic results of different line shapes. It is Oram's belief that the hand drawn nature of the lines make the results slightly inaccurate and to some extent unpredictable but in this also lies the possibility of bringing more humanity to the cold and precise machines generating the electronic signal.

*** UPIC
The UPIC ("Unité polyagogique informatique du CEMAMU") was a graphic sound synthesis system that was designed by Iannis Xenakis and arose from his graphic approach to composition. His earliest work, "Metastasis", completed was conceived using a graphic approach to describe trajectories and sound masses (figure fig:xenakis-metastatis). This approach has been attributed to his background in architecture, having worked in the studio of Le Corbusier. The UPIC was first conceived of in the seventies with the realisation of the first version in 1975 and its first public showcase in 1977. The work "Mycanae Alpha", composed in 1978 was the first work to use the system and was a  "nine-minute 38-second composition of dense and intense textures, of phase-shifting waveforms rich in harmonics that cascade, flutter, crash, and scream like sirens in a vast cosmological territory" \cite{tyranny_mycenae-alpha_2017}. 

This early version worked by drawing on a large digitizing graphics tablet which was interpreted by computer and converted into audio signals. The graphic approach to sound specification worked on a synthesis level by allowing the composer to draw and audition waveforms. Larger structures could be drawn in by switching to a "score" page and drawing lines, or "arcs" as they were denoted, on a pitch-time canvas. The final version of the application ran on personal computers and allowed for real-time interaction with a 64 oscillator synthesizer. At this stage, the input means had changed to a computer mouse but nevertheless retained the graphic approach of interaction. 

A primary goal of the UPIC project was that of pedagogy. Xenakis reasoned that the universality of sketching meant that it could provide an excellent teaching tool for a wide audience, even for young children (figure fig:xenakis-children). Another goal of the system was to encourage composer autonomy. At the time of its conception in the seventies, the technical barrier to entry into electronic music creation was very high and interfaces to help with this were rare or non-existent. Though the UPIC is not available to the general public currently, it has inspired a number of other systems that are available and will be considered in the next chapter. 

** A Golan Levin's AVES
Golan Levin created the interactive audio visual system, AVES, a series of audio visual installations in the late nineties and represented a landmark in the field of visual music. It is an attempt to move away from the diagrammatic approach to musical interfaces and to present an interface that is painterly in approach. Taking strong influence from visual artists such as Paul Klee, he presents a system that maps user input from a graphics tablet and mouse to visuals and to music. The intention is to create a strong visual correlation between the visuals and the music. A variety of approaches are taken to achieve this, all of them involving an algorithmic approach to one degree or another. For instance, in the piece "Aurora", he maps visuals of vast quantities of particles to a granulated sound synth sound source. He didn't take the approach of an exact mapping of visual particles to audio particles however and instead used a statistical control approach to approximate the correlation in between the visual and aural. (Levin, 2000)

For Levin, the digital pen input in combination with it's infinite variability represents an ideal instrument for creative expression in his digital temporal audio visual paintings. (???) The reason he gives for this is, similar to a musical instrument such as a violin, the pen is instantly knowable in that a child can pick it up and start creating marks but infinitely masterable through practice and hard work, and ultimately a vehicle for creative expression after a certain amount of mastery. A set of criteria that he and John Maeda arrived at to evaluate the success of their experiments was: is it instantly knowable, how long did you use it, how much of your personality can be expressed through it and, finally, with practice is it possible to improve using it. These ideas foreshadow much of the ideas of the Natural User Interface and echo principles in game design and music instrument design.

Levin's work is largely realtime and transitional in nature with gestures giving rise to visual and audio reactions that rise, fall and dissipate. A description that he uses of some of work is that of creating ripples in a pond. Therefore his work is very much geared towards an instrument like experience and is not concerned with the recording or visualization of the temporal unfolding of musical events as would be the function of compositional tools such as DAWs and musical notation. Indeed it is a conscious design decision to avoid such representations. Many of the principles and ideas of his work can, however, be applied in the context of a composition tool.

** William Coleman's sonicPainter
SonicPainter by William Coleman is a novel musical sequencer that seeks to address some of the shortcomings of traditional approaches to music sequencing found in commercial DAWs (Coleman, 2015). The focus of the line and node based interface (see figure) is to bring timbral shaping to the fore rather than being hidden away in miscellaneous automation lanes. The design takes influence from legacy musical systems, in particular, UPIC and incorporates ideas from visual music and embodied cognition.

Similarly to traditional sequencers, the x axis represents time and the y axis pitch. Note information is input via keyboard and mouse but more as an intermediary prototype stage. The default mode for input is to click to create a node and follow that with an additional click to continue to shape the note. The note can be ended by clicking a keyboard shortcut. By enabling the drawing notes as lines in this manner, the unfolding of the note can be explicitly represented visually. Other timbral aspects such as vibrato are represented by further visual manipulation of the line. For instance, an overlaid sine wave line indicates the timing and amplitude of the vibrato. In addition, the system allows for freehand input of notes.
Coleman recommends that the system could be further improved by multi-touch input, specific elements for the control of other synthesis techniques, time/pitch grid quantization, and further visual timbre feedback representations. Many of these recommendations will be addressed and explored in SonicSketch through a system built using the guidelines of NUI (natural user interface).

** Conclusion
While the dominant metaphors used in DAWs have their uses they can lead to limitations in the creative process particularly at the early stage of idea creation. More open system give too much power and impede the creative process. 
