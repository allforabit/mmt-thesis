<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article>
<article
  xmlns="http://docbook.org/ns/docbook" version="5.0"
  xmlns:xlink="http://www.w3.org/1999/xlink" >
  <info>
    <title></title>
  </info>
<para>
</para>
<para>
   
</para>
<para>
   
</para>
<para>
   
</para>
<section xml:id="introduction">
  <title>Introduction</title>
  <section xml:id="introduction-1">
    <title>Introduction</title>
    <para>
      The traditional route for both recorded and performed musical
      expression has been through playing a musical instrument. Through
      this simple but powerful interface, ideas can be explored by the
      skilled practitioner in a free flowing, intuitive manner. The
      ubiquity of high powered computers has led to a fundamental change
      to this process, however. Electronic music can now be produced
      entirely on a computer, enabling those without the background or
      training to produce music to a professional standard. The
      affordances of infinite editability and tweak-ability allows the
      novice producer to organically build compositions through a
      process of trial and error. The requirement to perform to a high
      standard is no longer necessary.
    </para>
    <para>
      The primary tool of this new breed of electronic musician, is the
      Digital Audio Workstation (DAW), a powerful software application
      for composing, arranging and editing musical events and audio
      material, mixing tracks and applying audio effects. To control
      this power, the electronic musician must master the vast array of
      knobs, sliders, buttons and complex hierarchies of settings. This
      complexity poses some difficult Human Computer Interaction (HCI)
      challenges both for software developers and end users (Duignan,
      Noble and Biddle, 2010).
    </para>
    <para>
      A common solution to this is to provide the user with real world
      metaphors in the presentation of the interface. In this way, the
      highly abstract process that takes place in digital systems can be
      grounded in a metaphorical language understandable to the user. An
      extremely dominant metaphor is that of the analog mixer and the
      hardware multi-track tape machine which have, since the mid-1990s,
      become a mainstay construct in the interfaces of commercial DAWs
      (Bell, Hein and Ratcliffe, 2015). The effectiveness of this
      metaphor is evident in the vast amount of music produced in this
      environments. There are some situations, however, where the
      metaphor can get in the way and lead to another new, separate
      layer of complexity that must, in turn, be managed by its users.
    </para>
  </section>
  <section xml:id="motivation">
    <title>Motivation</title>
    <para>
      This leads to the central motivation behind the work on
      SonicSketch: that of exploring alternative metaphors that support
      the early stage of music production and fulfills the role
      traditionally filled by an instrument. In exploring these
      alternative metaphors, an attempt is made to regain some of what
      is lost when working directly with the computer. At the same time,
      the distinct advantages of working in a digital medium will be
      maintained.
    </para>
    <para>
      The intention is not to suggest that analog studio metaphors can
      be replaced, or even that there is any reason that they should be.
      Instead, the intention is to augment these rich metaphors with
      other metaphors more suited to certain stages of the creative
      process. The need for such efforts is in some ways acknowledged by
      mainstream DAW manufacturers. For instance Ableton Live, a highly
      popular DAW application, has recently opened its application
      programming interface (API) to allow other programs to create
      compatible files (Ableton, 2017). This enables producers to work
      in diverse, idiosyncratic and perhaps less featured environments
      more suited to the capturing of ideas and creative flow. The
      option is open to then move freely to the more powerful fully
      featured environments offered by the DAW.
    </para>
  </section>
  <section xml:id="goals-and-objectives">
    <title>Goals and Objectives</title>
    <para>
      The overarching and primary goal of this project is to create a
      software application called SonicSketch that is specifically
      targeted at the early ideation part of the music production
      process. Building on a strong theoretical and technical foundation
      it will forego densely populated analog studio inspired DAW
      interface patterns and will instead focus on the metaphor of
      sketching. Users will draw various graphic symbols and lines on
      screen to produce sounds of different pitch, amplitude, and timbre
      which can be played back and altered in realtime.
    </para>
    <para>
      The final artifact should be:
    </para>
    <variablelist spacing="compact">
      <varlistentry>
        <term>
          Beginner friendly
        </term>
        <listitem>
          <para>
            the user should not need any lengthy explanation or
            instructions on its use but should be able to dive in and
            start making sounds straight away.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          User friendly
        </term>
        <listitem>
          <para>
            basic standard usability features should be added to reduce
            friction and allow users to engage more freely with the
            application.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          Expert friendly
        </term>
        <listitem>
          <para>
            flexible enough to allow users to use it for longer periods
            of time without getting tiring of it.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      These aspects will be assessed by using self-assessment as well as
      by testing with users. User testing will take the form of both
      interview and a standard usability test. To maximize availability
      and broaden the potential user base of testers, the application
      will be developed as a web application that will run in a modern
      browser without requiring the installation of any special
      software.
    </para>
  </section>
  <section xml:id="approach">
    <title>Approach</title>
    <para>
      The project is based upon and builds on SonicPainter, an
      application built by William Coleman for his Master's thesis.
      Similarly motivated by the overwhelming complexity of modern DAWs,
      the interface aims to be minimal and free of distraction. It
      presents a minimal canvas like space that allows the user to draw
      lines of various length, shape, and orientation resulting in
      sounds that vary in timing, duration, frequency, and timbre.
    </para>
    <para>
      Building on this existing work is advantageous in several ways. It
      provides a more concrete foundation than starting off with a blank
      slate. Features and approaches can be assessed so that certain
      features that work well can be incorporated and improved. Equally
      pitfalls in the original work can be avoided. A summary of
      intended improvements are as follows:
    </para>
    <itemizedlist spacing="compact">
      <listitem>
        <para>
          Increased discoverability of functions by adding user
          interface elements that enhance the &quot;sketch&quot;
          metaphor.
        </para>
      </listitem>
      <listitem>
        <para>
          Increased accessibility by making it available online.
        </para>
      </listitem>
      <listitem>
        <para>
          Improvements to usability such as allowing users to sketch in
          a freehand way more easily.
        </para>
      </listitem>
      <listitem>
        <para>
          Technical improvements to avoid crashes and unexpected
          application behaviour.
        </para>
      </listitem>
      <listitem>
        <para>
          Improve correlation between the visuals and the audio.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="thesis-structure">
    <title>Thesis structure</title>
    <para>
      This thesis begins with a discussion of the current dominant tools
      and practices in use in music production today with a strong focus
      on the ideation phase. A historical perspective is then given on
      more idiosyncratic approaches to music creation systems. An
      emphasis is given to systems that utilize a more visual approach.
      This is followed by a discussion of more recent work that to some
      degree takes influence from these approaches. The theoretical and
      practical approach that was taken in the build out of the project
      is then given and is followed by a more detailed technical
      walkthrough of how the system was put together. An evaluation of
      the success of the project is then given both from the perspective
      of the creator and from users that tested it. Finally, the broader
      implications of the work are discussed in addition to some
      suggestions for future research and development.
    </para>
  </section>
</section>
<section xml:id="background-sonic-sketching-as-an-alternative-metaphor">
  <title>Background: Sonic Sketching as an Alternative Metaphor</title>
  <section xml:id="introduction-2">
    <title>Introduction</title>
    <para>
      This chapter begins with a discussion on the dominant metaphors
      present in the modern DAW and expands on the earlier introduction
      to the topic. A concrete example is given to illuminate the issues
      and limitations that these metaphors can impose on its users. This
      is followed up with a survey of legacy systems that take a
      graphical approach to their interface and can be conceptually
      framed with the metaphor of sketching.
    </para>
  </section>
  <section xml:id="dominant-daw-metaphors">
    <title>Dominant DAW metaphors</title>
    <para>
      As has been asserted, analog studio metaphors of tape machines and
      hardware mixing desks dominate the UI approach to DAW interface
      design. Other prevalent metaphors often found in these interfaces
      are that of the <emphasis>outboard effects units</emphasis>, and
      the <emphasis>piano roll</emphasis> (Levin, 2000; Bell, Hein and
      Ratcliffe, 2015; Adenot, 2017; Ableton, 2017). A description of
      these now follows.
    </para>
    <section xml:id="multi-track-tape-recorder">
      <title>Multi-track tape recorder</title>
      <para>
        Multi track tape recording was introduced into the recording
        studios in the 1960s and is typified by the systems produced by
        Ampex and the Studer. These allowed producers to record multiple
        tracks of audio information which could be edited and using
        dubbing techniques mixed to taste. This unlocked significant
        creative possibilities and made albums like &quot;Sgt Peppers
        Lonely Heart Club Band&quot; possible. The underlying model of
        tracks typically manifests itself in DAWs as rectangular blocks
        stacked from top to bottom and running from left to right.
        Similar to editing tape, these can be spliced, cut, and pasted.
        Terms and techniques prevelant in DAWS like bouncing,
        overdubbing and markers (which were originally created using a
        physical pen), all have their roots in their analog precedents.
      </para>
    </section>
    <section xml:id="multi-channel-mixing-desk">
      <title>Multi channel mixing desk</title>
      <para>
        The multi channel mixing desk metaphor is present in the large
        majority of DAWs and is normally represented in a similar
        fashion to the sliders (or faders) found in hardware mixing
        desks (fig:mixing-desk). The mixing desk enabled the producer to
        control the relative amplitude of a finite amount of channels in
        addition to performing tasks such as panning to balance the
        signal in a stereo field. The slim vertical sliders found on
        most systems were codified in the 1960's by Bill Putnam and were
        created with ergonomics in mind (ref???). This layout allowed
        the producer to manipulate multiple channels of audio
        simultaneously in a practice known variously as &quot;riding the
        faders&quot; and &quot;playing the mixer&quot;. Despite the fact
        that the digital variants of these are largely controlled by a
        mouse that only affords the manipulation of a single fader at a
        time, they are still, largely speaking, presented in this
        fashion on screen.
      </para>
    </section>
    <section xml:id="outboard-effects-unit">
      <title>Outboard effects unit</title>
      <para>
        Outboard effects are hardware units used in studios to add audio
        effects to one or more channels on the mixing desk. The standard
        configuration of most studios allows for two different ways of
        applying these effects, by using insert effects and send
        effects. Insert effects are typically used when it only needs to
        affect a single channel, for instance, a chorus effect applied
        to an electric guitar. Send effects allow the producer to send a
        certain amount of the signal from a channel to specialised
        channels to perform processing on the signal in parallel with
        the original signal. It is typically used to apply effects that
        affect multiple channels of audio such as reverb effects. The
        introduction of Virtual Studio Technology (VST), by Steinberg,
        was responsible for bringing the outboard effects metaphor to a
        whole new height. This allowed third party developers to create
        virtual effects and instruments, and let producers expand their
        virtual studio beyond the built in effects. The visual
        interfaces increasingly paid homage to their hardware
        influences, emulating not only functionality but also the visual
        look (see figure fig:hardware-effects). (Levin, 2000) describes
        this as skeuomorphism, a design pattern where visual objects not
        only mimic real world objects in functionality but also
        incorporate unneeded visual features. The purpose of this is not
        only decorative but also educational, and gives connotational
        cues on how it should be used.
      </para>
    </section>
    <section xml:id="the-piano-roll">
      <title>The piano roll</title>
      <para>
        The piano roll is a primary metaphor found in almost all
        mainstream DAWS and is typically used to represent MIDI musical
        note information. MIDI, which stands for Musical Instrument
        Digital Interface is a standard protocol developed in the 1980s
        to allow control instructions to be sent between devices. It
        provided a standard language to for instance tell a synthesizer
        to play a particular note at a precise time and duration. These
        instructions could be collected into a MIDI file to, in effect,
        create a playable digital score. It is slightly distinct from
        the previously discussed examples in that it originates from a
        much earlier time period, the player pianos of the 1920s. The
        original piano rolls were operated by feeding a roll of paper
        with holes punched to indicate the precise timing that an
        attached piano should strike its notes. This provides an apt and
        suitable description for the MIDI musical data it normally
        represents. Similar to a player piano, no audible results are
        possible without an attached piano, and in the case of MIDI, an
        attached sound generating synthesizer device.
      </para>
    </section>
  </section>
  <section xml:id="a-compositional-example">
    <title>A compositional example</title>
    <para>
      Rather than discussing the issues that can arise from the
      metaphors in the abstract, let us consider a compositional idea
      and how we might achieve this in a DAW. The idea is broken into
      the following compositional &quot;recipe&quot;:
    </para>
    <orderedlist spacing="compact">
      <listitem>
        <para>
          Two notes of the same timbre are played together about an
          octave apart for a duration of 2 seconds.
        </para>
      </listitem>
      <listitem>
        <para>
          The first note glissandos to the frequency of the second note
          and vice versa.
        </para>
      </listitem>
      <listitem>
        <para>
          The first note starts with a small amount of vibrato that
          quickly dissipates.
        </para>
      </listitem>
      <listitem>
        <para>
          The second note starts with no vibrato but adds a small amount
          as the note nears completion.
        </para>
      </listitem>
      <listitem>
        <para>
          When these two notes end, the same pattern is repeated except
          this time with different timbres and frequencies.
        </para>
      </listitem>
      <listitem>
        <para>
          This is repeated 3 more times with different timbres and
          frequencies to complete this ten-second piece.
        </para>
      </listitem>
    </orderedlist>
    <para>
      While this may seem like a contrived example this, in fact,
      constitutes a compositional technique called Klangfarbenmelodie
      that involves splitting a melodic line between instruments or
      timbres to create a timbre melody. The glissandos and altering of
      vibrato intensity add further complexity and better illustrate
      some of the weaknesses inherent in DAW metaphors.
    </para>
    <section xml:id="realization-in-a-daw">
      <title>Realization in a DAW</title>
      <para>
        To achieve this in a DAW we have a few different options but a
        possible solution would be as follows:
      </para>
      <orderedlist spacing="compact">
        <listitem>
          <para>
            Working with the multitrack tape metaphor we can create ten
            separate tracks to house two different versions of each
            timbre. A vibrato plugin effect should be added to each of
            these by using a send or an insert effect. Two different
            tracks are needed for each of the timbres due to the fact
            that the two notes are played at the same time and both have
            different frequency and effect trajectories. If on the other
            hand, they had the same effect modulations or were played at
            different times, no additional tracks would be needed.
          </para>
        </listitem>
        <listitem>
          <para>
            Working with the piano roll metaphor, create a single note
            in each of these tracks setting each one to the desired
            fundamental frequency.
          </para>
        </listitem>
        <listitem>
          <para>
            Now edit the pitch bend automation lane by clicking into the
            relevant dialog
          </para>
        </listitem>
        <listitem>
          <para>
            Similarly, open the relevant dialog to edit the intensity of
            the vibrato effect
          </para>
        </listitem>
        <listitem>
          <para>
            Repeat this for each of the notes in the composition.
          </para>
        </listitem>
      </orderedlist>
      <para>
        At this point, we may have achieved what we set out to do.
        However, we now may want to tweak each of these elements to
        taste and perhaps add more material. An explosion in track count
        and overall complexity is inevitable. This can lead to a serious
        slowdown in workflow, a loss of flow and cognitive overload. A
        common technique to combat this complexity overload is to bounce
        the tracks and then continue working on these simpler artifacts
        (Duignan???). This, of course, negates a key advantage to
        working in a digital environment, the fine-grained ability to
        freely change, tweak and undo. Locating each note in separate
        tracks leads to an unnatural separation of what is, in fact,
        closely related compositional material. This requires awkward
        context switching and excessive navigation through the system to
        focus on different details.
      </para>
      <para>
        There are of course other tools in the DAW that may achieve this
        task more easily. For instance, a sampler may allow us to use
        different timbres on the same track and may work better in this
        case. We now have the extra task of exporting each of these
        samples in preparation for our composition work. Some other
        options present in many DAWS include aggregate instruments,
        multi-timbral instruments, and perhaps some midi routing
        options. Another option is to use an alternative, more flexible,
        environment such as an audio programming language. Some brief
        consideration of this will now be given.
      </para>
    </section>
    <section xml:id="realisation-in-code">
      <title>Realisation in code</title>
      <para>
        The piece could be realised in quite a straightforward manner in
        an audio programming language such as
        <emphasis>Csound</emphasis>. Central to
        <emphasis>Csound</emphasis> is the concept of the <emphasis>unit
        generator</emphasis> (or ugen), an abstraction to define both
        sound generators and processors. These can be patched together
        in a simple textual coding language to form instruments. A score
        is then specified, again in code, to define note onsets,
        durations in addition to other arbitrary parameters defined in
        the instruments. Each of the required timbres could have been
        represented as separate csound instruments, with each one
        configured with the desired timbre in addition to the vibrato
        effect. <emphasis>Function tables</emphasis> could be used to
        control the movement of the pitch glissando and the varying
        vibrato intensity. A function table is a list of numbers in
        Csound that can be read from, at various speeds, to supply
        control data to parameters (amongst other uses). A number of
        routines are available in Csound to generate commonly used list
        types. In this case, a line segment generator would be most
        applicable and would be used to generate a shape such as shown
        in fig:gen05. The Csound score would refer to each of the
        defined instruments with each note amounting to a single line of
        code, making the entire score a total of five lines.
        Demonstration code is provided in the appendix.
      </para>
      <para>
        Depending on the experience of the reader, this may or may not
        seem like a better approach than using the DAW owing to the
        following central issue. It is not beginner friendly and a
        reasonable amount of prior experience and/or training is
        required. Perhaps a bigger criticism that could be made,
        however, is that it can lead to an analytical rather than a
        creative way of thinking. In &quot;Thinking Slow, Acting
        Fast&quot;, Daniel Daniel Kahneman contrasts these two ways of
        thinking which he terms <emphasis>System 1</emphasis> and
        <emphasis>System 2</emphasis>. System 1 is instinctive, fast,
        emotional and is a mode of thinking that may not register
        consciously. System 2 is slow, logical, analytical and registers
        prominently in active consciousness. Routine tasks such as
        walking, opening doors etc only use system 1 thinking. These can
        be completed while exerting minimal cognitive effort (all the
        while calculating the complex motor sensory actions that must
        take place). Complex analytical tasks such as programming
        require system 2 thinking. Approaching creative tasks such as
        music making in this way where instinct and emotion are often
        crucial can slow down or stop the process. Perhaps it is best
        summed by John Cage: &quot;Don't try to create and analyse at
        the same time. They're different processes&quot; (Popova, 2012).
      </para>
    </section>
  </section>
  <section xml:id="sketching-as-an-alternative-metaphor">
    <title>Sketching as an alternative metaphor</title>
    <para>
      While audio programming languages are an abstraction over more
      complex underlying computational processes, they largely speaking
      offer a model that is closer to these processes than the more
      abstracted DAW interfaces. As we have discussed, though what is
      gained in flexibility can be lost in intuitiveness and ease of
      interaction. Rather than discarding these higher level metaphors,
      perhaps a better approach would be to explore alternate metaphors.
    </para>
    <para>
      A rather promising but nonmainstream approach is that of sonic
      sketching. This has a long and illustrious historical precedent
      reaching back well before the, now more prevalent, studio
      metaphors. As is pointed out by Levin (2002), the exploration of
      synchrony between audio and visuals is a practice going back
      centuries and was variously termed &quot;ocular music, visual
      music, color music, or music for the eyes&quot; (Levin, 2000). The
      twentieth-century technique of the optical soundtrack, however,
      brought these ideas to a new level of sophistication. The
      technique, which involved placing marks via photography or direct
      manipulation to specify audio properties, was explored by such
      luminaries as Oskar Fischinger, Norman McLaren and Daphne Oram.
      Oram's particular take on the technique will now be discussed.
    </para>
    <section xml:id="oramics">
      <title>Oramics</title>
      <para>
        A primary motivating factor behind Daphne Oram's development of
        the Oramics machine was to bring more human-like qualities to
        the sounds generated by electronic means. The machine worked by
        playing back multiple lanes of film tape in unison, defining a
        monophonic series of notes as well as control signals to shape
        their timbre, pitch and amplitude. She details the thought
        process behind this in her hugely insightful and broad ranging
        journal style book, &quot;An Individual Note&quot;
        (<emphasis role="strong">???</emphasis>).
      </para>
      <para>
        The aspects of the sound that she wishes to control are volume,
        duration, timbre, pitch, vibrato, and reverb. In order to do
        this, she describes a simple musical notation language based on
        the freehand drawing of lines combined with discrete symbols.
        The lines, which she describes as the analog control, are used
        to define volume envelopes. Interestingly, the default and
        preferred method for the parameters she wishes to control is the
        continuous line rather than discrete note symbols. For instance,
        she avoids the use of a static velocity per note and instead
        only specifies the use of a control envelope to change
        amplitude.
      </para>
      <para>
        The discrete symbols, which she categorizes as digital control,
        are used to define individual pitches and are termed neumes. She
        highlights that notes should not remain static and, thusly, an
        analog control of each note is also specified. Similarly to
        amplitude and vibrato, timbre is also defined by the freehand
        drawing of lines and is something that with practice the
        &quot;inner ear&quot; can develop an intuition as the sonic
        results of different line shapes. It is Oram's belief that the
        hand drawn nature of the lines make the results slightly
        inaccurate and to some extent unpredictable. Herein, however,
        lies the possibility of bringing more humanity to the cold and
        precise machines generating the electronic signal.
      </para>
    </section>
    <section xml:id="upic">
      <title>UPIC</title>
      <para>
        The UPIC (&quot;Unit√© polyagogique informatique du CEMAMU&quot;)
        was a graphic sound synthesis system that was designed by Iannis
        Xenakis and arose from his graphic approach to composition. His
        earliest work, &quot;Metastasis&quot;, was conceived using a
        graphic approach to describe trajectories and sound masses
        (figure fig:xenakis-metastatis). This approach has been
        attributed to his background in architecture, having worked in
        the studio of Le Corbusier. The UPIC was first conceived of in
        the seventies with the realisation of the first version in 1975
        and its first public showcase in 1977. The work &quot;Mycanae
        Alpha&quot;, composed in 1978 was the first work to use the
        system and was a &quot;nine-minute 38-second composition of
        dense and intense textures, of phase-shifting waveforms rich in
        harmonics that cascade, flutter, crash, and scream like sirens
        in a vast cosmological territory&quot;
        (<emphasis role="strong">???</emphasis>).
      </para>
      <para>
        This early version worked by drawing on a large digitizing
        graphics tablet which was interpreted by a high-powered computer
        (for that period) and converted into audio signals. The graphic
        approach to sound specification worked on a synthesis level by
        allowing the composer to draw and audition waveforms. Larger
        structures could be drawn in by switching to a &quot;score&quot;
        page and drawing lines, or &quot;arcs&quot; as they were
        denoted, on a pitch-time canvas. The final version of the
        application ran on personal computers and allowed for real-time
        interaction with a 64 oscillator synthesizer. At this stage, the
        input means had changed to a computer mouse but nevertheless
        retained the graphic approach of interaction.
      </para>
      <para>
        A primary goal of the UPIC project was that of pedagogy. Xenakis
        reasoned that the universality of sketching meant that it could
        provide an excellent teaching tool for a wide audience, even for
        young children (figure fig:xenakis-children). Another goal of
        the system was to encourage composer autonomy. At the time of
        its conception in the seventies, the technical barrier to entry
        into electronic music creation was very high and interfaces to
        help with this were rare or non-existent. Though the UPIC is not
        available to the general public currently, it has inspired a
        number of other systems that are available.
      </para>
    </section>
    <section xml:id="a-golan-levins-aves">
      <title>A Golan Levin's AVES</title>
      <para>
        Golan Levin created the interactive audio-visual system, AVES, a
        series of audio visual installations in the late nineties and
        represented a landmark in the field of visual music. It is an
        attempt to move away from the diagrammatic approach to musical
        interfaces and to present an interface that is painterly in
        approach. Taking strong influence from visual artists such as
        Paul Klee, he presents a system that maps user input from a
        graphics tablet and mouse to visuals and audio. The intention is
        to create a strong visual correlation between these two
        modalities. A variety of approaches are taken to achieve this,
        all of them involving an algorithmic approach to a certain
        degree. For instance, in the piece &quot;Aurora&quot;, he maps
        visuals of vast quantities of particles to a granulated sound
        synth sound source. He didn't take the approach of an exact
        mapping of visual particles to audio particles, however, and
        instead used a statistical control approach to approximate the
        correlation in between the visual and aural. (Levin, 2000)
      </para>
      <para>
        For Levin, the digital pen input in combination with it's
        infinite variability represents an ideal instrument for creative
        expression in his digital temporal audio visual paintings. (???)
        The reason he gives for this is, similar to a musical instrument
        such as a violin, the pen is instantly knowable in that a child
        can pick it up and start creating marks but infinitely
        masterable through practice and hard work, and ultimately a
        vehicle for creative expression after a certain amount of
        mastery. A set of criteria that he and John Maeda arrived at to
        evaluate the success of their experiments was: is it instantly
        knowable, how long did you use it, how much of your personality
        can be expressed through it and, finally, with practice is it
        possible to improve using it.
      </para>
      <para>
        Levin's work is largely realtime and transitory in nature with
        gestures giving rise to visual and audio reactions that rise,
        fall and dissipate. A description that he uses of some of work
        is that of creating ripples in a pond. Therefore his work is
        very much geared towards an instrument like experience and is
        not concerned with the recording or visualization of a score or
        timeline of musical events as would be the function of
        compositional tools such as DAWs. Indeed it is a conscious
        design decision to avoid such representations. Many of the
        principles and ideas of his work can, however, be applied in the
        context of a composition tool.
      </para>
    </section>
    <section xml:id="william-colemans-sonicpainter">
      <title>William Coleman's sonicPainter</title>
      <para>
        SonicPainter by William Coleman is a novel musical sequencer
        that seeks to address some of the shortcomings of traditional
        approaches to music sequencing found in commercial DAWs
        (Coleman, 2015). The focus of the line and node based interface
        (see figure) is to bring timbral shaping to the fore rather than
        being hidden away in miscellaneous automation lanes. The design
        takes influence from legacy musical systems, in particular, UPIC
        and incorporates ideas from visual music and embodied cognition.
      </para>
      <para>
        Similarly to traditional sequencers, the x axis represents time
        and the y-axis, pitch. Note information is input via keyboard
        and mouse. A click starts a note and can be followed with
        additional clicks to continue to shape it. It can be ended by
        clicking a keyboard shortcut. By drawing notes as lines in this
        manner, the unfolding of the note can be explicitly represented
        visually. Other timbral aspects such as vibrato are represented
        by further visual manipulation of the line. For instance, an
        overlaid sine wave line indicates the timing and amplitude of
        the vibrato. In addition, the system allows for freehand input
        of notes. Coleman recommends that the system could be improved
        by adding multi-touch input, allowing for other synthesis
        techniques, time/pitch grid quantization, and further visual
        timbre feedback representations.
      </para>
    </section>
  </section>
  <section xml:id="conclusion">
    <title>Conclusion</title>
    <para>
      The dominant metaphors present in DAWs, which are by and large
      analog studio influenced were discussed including details on their
      origins and their reincarnation in digital form. A short
      compositional example was given and the process to realise this in
      a DAW was described. The piano roll, multi-track mixer, and
      outboard effects metaphors were shown to be a poor fit for this
      particular compositional idea and resulted in an excessive amount
      of tracks and, therefore, complexity. A simpler solution was
      described in the csound audio programming environment. The lower
      level abstractions provided here allowed for a more succinct and
      simpler implementation of the piece. Some potential pitfalls to
      this approach were given. This includes a steep learning curve for
      novice users and a potential bias towards an analytical rather
      than a creative mode of thinking. Rather than abandoning the
      high-level metaphors present in DAWs it was posited that another
      approach could be to explore other metaphors more suited to
      certain compositional ideas. To this end, the metaphor of
      sketching as an interface to audio systems was explored by tracing
      it's early roots in the optical soundtracks of Oram to the
      realtime synth sketching of Xenakis's UPIC through to the
      contemporary approaches of Golan Levin's AVES system and William
      Coleman's SonicPainter.
    </para>
  </section>
</section>
<section xml:id="my-approach">
  <title>My approach</title>
  <section xml:id="introduction-3">
    <title>Introduction</title>
    <para>
      The following chapter opens with an appraisal of currently
      available graphical synthesis systems that were discussed in the
      previous chapter to more clearly define the niche that SonicSketch
      seeks to fill. Some theory behind the development is then given
      including HCI considerations, the Musical Interface Technology
      Design Space and research into cross modal perception. The
      practical approach is then given which delves into the more
      technical aspects of the project build out including a discussion
      on the Web Audio Api, <emphasis>tone.js</emphasis>,
      <emphasis>paper.js</emphasis>, <emphasis>ClojureScript</emphasis>
      and <emphasis>react.js</emphasis>.
    </para>
  </section>
  <section xml:id="appraisal-of-options">
    <title>Appraisal of options</title>
    <para>
    </para>
  </section>
  <section xml:id="approach---theory">
    <title>Approach - theory</title>
    <section xml:id="hci-considerations">
      <title>HCI considerations</title>
      <orderedlist>
        <listitem>
          <para>
            The natural user interface
          </para>
          <para>
            NUI is an evolution of the concept of the graphic user
            interface and refers to an approach to human computer
            interaction beyond that of the traditional keyboard and
            mouse or what has been termed the WIMP model. It encompasses
            a set of guidelines and best practices which are set out
            most comprehensively by Wigdor (2014). Some of the basic
            tenets of NUI are as follows:
          </para>
          <itemizedlist spacing="compact">
            <listitem>
              <para>
                Harness existing skills when possible without
                necessarily mimicing the real world tool or instrument
                (Wigdor and Wixon, 2011, p. 13).
              </para>
            </listitem>
            <listitem>
              <para>
                Be friendly and learnable by beginners but allow for
                mastery given enough practice (a sentiment shared by
                Levin, above) (Wigdor and Wixon, 2011, p. 13).
              </para>
            </listitem>
            <listitem>
              <para>
                Immediate feedback for all interactions should take
                place, most usually but not limited to, visual feedback.
                (Wigdor and Wixon, 2011, p. 87)
              </para>
            </listitem>
          </itemizedlist>
          <para>
            Furthermore, the interface should take advantage of the
            particular affordances offered by the input method. (Wigdor
            and Wixon, 2011, p. 115) An apt example of this is the early
            introduction of digital pens for windows laptops where the
            pen wasn't suited to the WIMP interface and failed to
            receive widespread usage. In this case, the interface forces
            the user to carry out awkward gestures for the medium,
            including double clicking and right clicking, failing to
            take advantage of the stroke gesture much more suited to it.
          </para>
          <para>
            The system CrossY, referenced in Wigdor (2014), uses a cross
            gesture stroke to interact with buttons, menus, and widgets,
            as well as the painting functionality
            (<emphasis role="strong">???</emphasis>). The CrossY gesture
            system enables the user to, for instance, select brush size
            and colour in one stroke by dragging the pen from right to
            left across an icon and validating selection by dragging
            past the left or bottom selected leaf icon. This is
            illustrated clearly in the left-most diagram of the provided
            figure (fig. 3).
          </para>
        </listitem>
      </orderedlist>
    </section>
    <section xml:id="the-musical-interface-technology-design-space-mitds">
      <title>The Musical Interface Technology Design Space
      (MITDS)</title>
      <para>
      </para>
    </section>
    <section xml:id="cross-modal-perception">
      <title>Cross modal perception</title>
      <para>
      </para>
    </section>
  </section>
  <section xml:id="approach---practice">
    <title>Approach - practice</title>
    <section xml:id="delivery-on-web-browser">
      <title>Delivery on Web Browser</title>
      <para>
      </para>
    </section>
    <section xml:id="modern-web-browser-as-a-delivery-platform">
      <title>Modern web browser as a delivery platform</title>
      <para>
      </para>
    </section>
    <section xml:id="benefits-of-using-tone.js-citemann_interactive_2015">
      <title>Benefits of using tone.js Mann (2015)</title>
      <para>
      </para>
    </section>
    <section xml:id="paper.js-for-the-graphics-system">
      <title>Paper.js for the graphics system</title>
      <para>
      </para>
    </section>
    <section xml:id="fm-synthesis">
      <title>FM synthesis</title>
      <para>
      </para>
    </section>
    <section xml:id="live-coding-workflow">
      <title>Live coding workflow</title>
      <orderedlist>
        <listitem>
          <para>
            [Introduction]
          </para>
        </listitem>
        <listitem>
          <para>
            React.js framework
          </para>
          <para>
            react is a web framework built by facebook that aids the
            developer in updating the dom (document object model), a
            process that is required when the state of teh applciation
            changes. this was a role traditionally carrried out on the
            server and served to users as a static page. this all
            changed however with the rise of single page applications
            (spa) around the 2???s. the value proposition of the spa is
            increased interactivity and responsives to user input,
            allowing the look and contents of the page to update
            dynamically as the user interacst with the system. to aid in
            the construction of these spa's a number of frameworks to
            help the process were introduced by the open source
            community. some popular early examples include
            <emphasis>backbone.js</emphasis> and
            <emphasis>angular.js</emphasis>. a technique that saw some
            popularity was a system called two way binding which created
            two way link between the current state in the model and the
            visual appearance of the view. this however has a number of
            issues including some serious performance issues, in
            addition to some conceptual problems (???ref). react offers
            a simpler one way bidning ssytem using what is termed the
            virtual dom. in this model a special virtual version of the
            dom is constructed and when the model changes is updated.
            the parts of the dom that require changing can thusly be
            pinpointed and the real dom can be efficiently updated. this
            system has proven to be particulalrly beneficial when paired
            with functional programming techniques, a style of
            programming that emphasizes the use of pure functions as the
            primary building block of programs. in the case of working
            with the dom, it can lead to not only an increase in
            efficiency in the rendering of the applications but also a
            simplification of the programming model. a number of
            projects have emerged that attempt to bring this benefits of
            the react model beyond the realm of the dom including
            writing console prgorams (???ref), writing web audio
            applications (???ref) and even arduino projects (???ref).
          </para>
        </listitem>
        <listitem>
          <para>
            Clojurescript
          </para>
        </listitem>
        <listitem>
          <para>
            Managing state with re-frame
          </para>
        </listitem>
      </orderedlist>
    </section>
  </section>
  <section xml:id="conclusion-1">
    <title>Conclusion</title>
    <para>
    </para>
  </section>
</section>
<section xml:id="execution">
  <title>Execution</title>
  <section xml:id="introduction-5">
    <title>Introduction</title>
    <para>
      The following chapter gives an outline of the process that was
      undertaken to build out the final application. A description of
      early prototype work is given to give context to the construction
      of the final working prototype version. This is followed by a
      description of the technical architecture of the system as well as
      an outline of the sometimes tricky setup process of getting the
      live reload system working (as described in the previous chapter).
      Some detailed discussion of the the core functionality of the
      system is then given. The core of the system primarily consists of
      timeline events, which visually manifest themselves as strokes on
      the canvas and aurally as fm syntesized frequency modulated
      sounds. As will be discussed an important aspect of any well
      constructed software system is a good degree of seperation of
      concerns, a characteristic espoused to and evident in the
      resulting code. To this end, the code that brings the central
      functionality to life can conceptually divided into a data or
      entity layer, a business logic or use case layer and an output
      layer which in this case consists of the visual output into a html
      canvas element and the audio output through the web audio api. In
      this sense the architecture conforms to the principles of the
      Clean Architecture as presented by Bob Martin (Martin, 2012).
      (<emphasis role="strong">???</emphasis>_clean_2012)
    </para>
  </section>
  <section xml:id="early-prototype-work">
    <title>Early prototype work</title>
    <section xml:id="melodypainter">
      <title>Melodypainter</title>
      <para>
        Melodypainter is an early protoype built out in Max MSP that
        allows users to draw freehand lines, which are converted into
        break point function data and used to generate a melodic
        profiles using Bach for Max MSP. Bach is a suite of composition
        tools that allow for a number of computer aided composition
        techniques (CAC) and provides similar functionality to IRCAM's
        Open Music system. These melodic profiles are then filtered to
        only includes notes from a pentatonic scale, to give reasonably
        pleasing aural results. Some notable flaws in the system include
        the following. It is limited to strictly western tonal music
        styles. It has no allowance for rhythm and plays only eight
        notes giving results a noticeably bland and predictable quality.
        The freeform nature of sketched input however was quite a
        pleasing means of inputting the control information.
      </para>
    </section>
    <section xml:id="sonicshaper-01">
      <title>Sonicshaper [0/1]</title>
      <para>
        A separate application was created in Processing which allowed
        users to draw shapes, using either mouse or ideally, pen input
        and have a sound that is associated with each shape played back.
        As the sound of each shape plays back, it is lit up using
        animation, creating a strong connection between the shape and
        it's resulting sound. The application uses the &quot;gesture
        variation follower&quot; system (Caramiaux <emphasis>et
        al.</emphasis>, 2015), which while promising in principle,
        didn't have a high rate of accuracy in recognizing the shapes.
      </para>
    </section>
    <section xml:id="web-version-of-william-colemans-sonicpainter-01">
      <title>TODO Web version of William Coleman's SonicPainter
      [0/1]</title>
      <para>
        A potential starting point that was considered was using the
        code from William's SonicPainter and porting it to the web
        platform. This process proved to be quite straightforward. The
        processing code could more or less be embedded in a webpage as
        is using &quot;processing.js&quot;, a web version of the
        Processing library that enables users to run processing sketches
        in the Web Browser. Some notable changes that had to be made
        were removing the OSC functionality as this is not technically
        possible to use in a browser. In addition, some other pieces of
        code had to be commented out and tweaked. As it's not possible
        to run Max MSP patches in the browser, the audio system was
        re-implemented using Tone.js. As SonicPainter uses simple FM
        synthesis, a very close approximation to the original version
        could be created. In the end, it was decided not to build on
        this codebase however as there were some issues with
        functionality and usability that would be difficult to resolve
        in an inherited codebase. A fundamental issue was that
        transitions between certain states would cause crashes or
        unpredicatable behaviour. An example of this is when a user
        attempts to use the vibrato tool while in the process of
        creating a note. Instead of either finishing the note and
        starting the vibrato tool or disallowing the behaviour, the
        program would crash. This is a common problem in software
        development and is evidence even in commercial products. To
        alleviate such issues in the new codebase a more disciplined
        approach would be taken to managing transitions between states.
        The process of porting the code did however give a more in depth
        incite into Coleman's implementation incuding the pitfalls
        mentioned above. In addition, the basic visual look and
        conceptual functionality would form the basis of the workings of
        SonicSketch.
      </para>
    </section>
  </section>
  <section xml:id="actual-implementation">
    <title>Actual implementation</title>
    <section xml:id="setting-up-the-architecture">
      <title>Setting up the architecture</title>
      <orderedlist>
        <listitem>
          <para>
            Clojurescript and javascript npm modules
          </para>
          <para>
            Despite the fact that clojurescript has existed for six
            years(???), some areas of the development process are still
            difficult, particularly when building out a more complex
            real world application. It should be noted that a good deal
            of work is being carried out to make this a smoother
            experience and thusly these pains are likely to become less
            of an issue in the near future (???ref). It should also be
            noted that building applicatons using plain javascript is
            not a trivial process and in all likelyhood will include a
            build process using a system like webpack or browserify. A
            primary issue that had to be resolved to allow the
            application to be built out was the incorporation of
            javascript npm modules. NPM is the module system used by
            node.js originally for more server oriented technologies but
            increasingly for rich clientside applications. For a purely
            javascript application, it would be a matter of simply
            adding the desired libraries as dependencies. However, with
            the use of clojurescript some extra steps needed to be
            carried out. In addition to adding the dependencies, a
            javascript file was created that imported these into a
            &quot;deps&quot; object. This deps object could then be
            referred to in clojurescript using the standard interop
            syntax <literal>js/deps.myDependency</literal>. At the time
            of development an alpha feature that allowed npm
            dependencies to be declared as part of the project.clj file
            was experimented with but was not used in due to
            difficulties getting it to work. While the project setup was
            not as elegant or succint as might be wished, it did provide
            a stable base to build on and a means to harness the rich
            resource that is the NPM ecosystem and use such tools as
            Paper.js and React.js.
          </para>
        </listitem>
        <listitem>
          <para>
            Paper.js and react.js (paper.js bindings)
          </para>
          <para>
            As has been outlined in the previous chapter, a declarative
            coding style would be employed to enable a live coding
            workflow and to avoid a building a codebase that is
            increasingly difficult to understand. In other words the
            code should as much as possible describe the
            &quot;what&quot; of the functionality rather than the
            &quot;how&quot;. These qualities emerge quite naturally when
            using the <emphasis>React.js</emphasis> architecture.
            Paper.js however runs in the context of a canvas element and
            thusly it is not possible to directly use
            <emphasis>React.js</emphasis> with it. This shortcoming has
            been addressed in projects such as <emphasis>three.js react
            bindings</emphasis> and <emphasis>pixi.js react
            bindings</emphasis> which allow the use of react's
            declaritive programming style for 3d and 2d scenegraph
            oriented systems that run in the html canvas element. These
            solutions both work by creating dummy empty dom elements and
            hook into the <emphasis>React.js</emphasis> lifecycle events
            to the real work of updating the scenegraph. In many ways
            the scene graph structure of projects like these and indeed
            Paper.js exhibit a high resemblance to DOM structures and
            APIs making React a good fit for them. A similar approach to
            the above mentioned libraries approach was taken to
            integrate paper.js for use in SonicSketch and worked
            reasonably well but required quite a bit of setup. During
            the development of the project, a more suitable solution
            emerged from the open source community at an opportune time.
            This used the next version of <emphasis>React.js</emphasis>
            which has better support for render targets that are not the
            DOM and has the distinct advantage of not requiring the
            creation of redundant DOM nodes. The library was far from
            comprehensive and thusly a custom version of the library was
            used that included some custom functionality required for
            SonicSketch.
          </para>
        </listitem>
        <listitem>
          <para>
            Tone.js and react.js
          </para>
          <para>
            In some ways audio output can be thought of in a similar way
            to the visual output of the app and thusly can be treated in
            similar way by <emphasis>React.js</emphasis>. It can use the
            declarative data oriented system of react to configure the
            particular settings and connections in the audio graph and
            hook in to its lifecycle events to instanciate the various
            audio generating and processing web audio nodes. This
            addresses a notable (by design) ommission in Tone.js which
            does not allow the code to query the state of the audio
            graph once it has been setup. It is down to the userland
            code to keep track of this and manage it accordingly. The
            value proposal offered by introducing react.js into this
            part of the system is that it maintains the simple
            relationship between state and generated output.
            Conceptually the flow of change is:
          </para>
          <orderedlist spacing="compact">
            <listitem>
              <para>
                The state updates
              </para>
            </listitem>
            <listitem>
              <para>
                The react wrapper objects update their properties
                accordingly
              </para>
            </listitem>
            <listitem>
              <para>
                The lifecycle events are triggered which takes care of
                altering, adding and removing web audio nodes (thus
                altering the audio being output)
              </para>
            </listitem>
          </orderedlist>
          <para>
            The design of this part of the application is influenced by
            <emphasis>react music</emphasis>, a system that uses
            <emphasis>React.js</emphasis> with tuna.js, a web audio
            library similar to tone.js (???ref).
          </para>
        </listitem>
        <listitem>
          <para>
            Reagent and react.js paper.js bindings
          </para>
          <para>
            The final piece of the jigsaw in the underlying technology
            stack is the integration of react with clojurescript via the
            <emphasis>Reagent</emphasis> library. The core syntax of
            this system is simple clojurescript vectors similar to the
            following:
          </para>
          <programlisting language="clojure">
[:div 
 &quot;Hello &quot; [:span {:style {:font-weight bold}}
world]]
</programlisting>
          <para>
            This would result in the following html output:
          </para>
          <programlisting language="html">
&lt;div&gt;Hello &lt;span style=&quot;font-weight: bold&quot;&gt;world&lt;/span&gt;&lt;/div&gt;
</programlisting>
          <para>
            As can be seen the vectors begin with a keyword that
            corresponds to the tagname of the html. Additionally,
            instead of using html tag keywords, function calls can be
            made to generate html to allow for code reuse and logic. It
            was unclear how the paper.js bindings would work within this
            system due to the fact that it required a different version
            of react and uses non standard tag names for elements that
            can be drawn on screen such as &quot;circle&quot; and
            &quot;rectangle&quot;. This however turned out to be much
            more straightforward than expected and the provided paper.js
            primitives could by simply using the relevant paper.js
            keywords. Complex scenegraphs could be constructed by using
            the following succint clojurescript syntax to describe the
            playback indicator:
          </para>
          <programlisting language="clojure">
[:Group {:position [position 0]
           :pivot [0 0]
           :opacity    0.75}
   [:Rectangle {:pivot [0 0]
                :size [1 height]
                :fill-color &quot;#ffffff&quot;}]
   [:Path {:segments     [[-5 0] [5 0] [0 7] [-5 0]]
           :fill-color &quot;#ffffff&quot;}]]
</programlisting>
          <para>
            As can probably be inferred from the code the
            <literal>position</literal> and <literal>height</literal>
            are properties that are passed into the hiccup and trigger
            updates to the visual display when they change: in the case
            of position, when the playback position changes and in the
            case of the height, when the user resizes the browser
            window. The path element describes the triangle that is
            places at the top of the screen.
          </para>
          <para>
            The current state of the art in live code reloading in the
            browser is still not as comprehensive or as easy to setup as
            might be wished. Once it has been configured it is difficult
            to return to the compile and run workflow, and is in most
            cases a worthwhile investment of time. With these underlying
            elements in place the process of creating the core
            functionality application could begin and will now be
            described.
          </para>
        </listitem>
      </orderedlist>
    </section>
    <section xml:id="core-functionality---timeline-events-or-notes">
      <title>Core functionality - timeline events (or notes)</title>
      <para>
        At the core of the application is the creation of timeline
        events which unfold in a looped fashion. These events are
        created based on the input of the user with a mouse or mouse
        like input device. On the production of a valid input gesture,
        the screen is updated immediately with a visual display of this
        content. The details of this gesture is stored in memory and the
        event that will eventually create the sound is registered with
        tone.js. Much of the events that occur in the system are
        captured in a main &quot;View&quot; component which houses the
        central html canvas element. To aid in organising the large
        amount of functionality associated with the component, higher
        order components are used to separate this out into logical
        groupings. A higher order component is a component that wraps a
        normal component to add functionality to it and accepts the same
        properties as the component it wraps (???ref-react). In this
        case the most logical grouping is by tool and so there are
        higher order components setup for each of the tools: draw,
        vibrato, delete, move, resize and probability.
      </para>
      <orderedlist>
        <listitem>
          <para>
            Add timeline event
          </para>
          <para>
            This is the default tool that is activated when the user
            opens the application and enables the user to add timeline
            events by drawing them onto the screen. It is added to the
            system when the &quot;draw&quot; tool is activated and a
            mouse drag operation is carried out from left to right
            within the bounds of the canvas element. The event is
            captured in the main canvas view and is initiated when the
            user left clicks the mouse triggering the following
            function:
          </para>
          <programlisting language="clojure">
(defn pointer-down [{:keys [temp-obj active-preset]} evt]
  (let [pointer-point (.. evt -point)
        group         (js/paper.Group. (clj-&gt;js {:position 
                        [(.. pointer-point -x) (.. pointer-point -y)]
                                                 :applyMatrix false
                                                 :pivot [0 0]}))
        circle        (js/paper.Shape.Circle. (clj-&gt;js {:fillColor &quot;#ffffff&quot;
                                                        :radius    5}))
        path          (js/paper.Path. (clj-&gt;js {:strokeColor   &quot;#ffffff&quot;
                                                :strokeWidth   2
                                                :fullySelected true
                                                :segments      [[0 0]]}))]
    (.. group (addChildren #js [circle path]))
    (reset! temp-obj {:path   path
                      :circle circle
                      :group  group
                      :loc pointer-point})))
</programlisting>
          <para>
            This function receives a hashmap with a reference to a
            <emphasis>ClojureScript</emphasis> atom which can is
            <literal>reset!</literal> to contain a temporary
            visualisation of the newly created note. This function uses
            a heavy amount of javascript interop to directly instanciate
            paper.js objects and add them to a shared group.
          </para>
          <para>
            As the user continues to move the cursor further points are
            added to the path created in the
            <literal>pointer-down</literal> function. Some constraints
            however are placed on the creation of the path and only
            points that are past the last previous from left to right
            are added. If the users backtracks it lead to a deletion of
            points, providing an intuitive undo like behaviour and
            implementing a recipricol HCI interaction pattern
            recommended by NUI principles.
          </para>
          <programlisting language="clojure">
(defn pointer-move [{:keys [temp-obj active-preset]} evt]
  (when-let [{:keys [path group] :as temp-obj} @temp-obj]
    (let [pointer-point (.. evt -point)
          rel-pos       (.. group (globalToLocal pointer-point))]
      ;; Only add positive points relative to first
      ;; Remove points greater than pointer-points
      (when-let [last-seg (.. path getLastSegment)]
        (let [first-seg   (.. path getFirstSegment)
              first-point (-&gt; first-seg .-point)
              last-point  (-&gt; last-seg .-point)
              pointer-x   (.-x rel-pos)
              amp-env     (-&gt; active-preset :envelope)
              stage-width (.. evt -tool -view -viewSize -width)
              max-width   (if (= (-&gt; amp-env :sustain) 0)
                            (let [time (+ (-&gt; amp-env :attack)
                                          (-&gt; amp-env :decay)
                                          (-&gt; amp-env :release))]
                              (-&gt; time
                                  ;; Seconds to beats
                                  (* (/ js/Tone.Transport.bpm.value 60))
                                  (time-&gt;euclidian stage-width)))
                            nil)]
          (when (or
                 (nil? max-width)
                 (&lt; pointer-x (+ (.-x first-point) max-width)))
            (-&gt; path (.add rel-pos))
            (let [greater-segs (filter
                                #(&gt; (-&gt; % .-point .-x) pointer-x)
                                (.-segments path))]
                ;; Remove greater points
              (doseq [seg greater-segs]
                  (.removeSegment path (.-index seg))))))))))
</programlisting>
          <para>
            Completion of a note occurs when the user releases the
            button and triggers the <literal>pointer-up</literal>
            function:
          </para>
          <programlisting language="clojure">
(defn pointer-up [{:keys [temp-obj active-preset stage-size]} evt]
  (let [{:keys [path circle group loc] :as temp-obj} @temp-obj]
    (.simplify path 10)
    ;; Send the actual note
    (dispatch [:note-add (-&gt; (path-&gt;note path loc stage-size)
                             (assoc ,,, :preset active-preset)
                             ;; Use the color from the active preset
                             (assoc ,,, :color (:color active-preset)))] )
    ;; Remove temp stuff
    (.remove path)
    (.remove group)
    (.remove circle))
  ;; Unset temp obj
  (reset! temp-obj nil))
</programlisting>
          <para>
            This function simplifies the path by calling the paper.js
            <literal>simplify</literal> method on the path object and
            dramatically reduces the amount of data captured while
            preserving the basic characteristic of the user's stroke
            (???ref-simplify-fn). Most importantly it calls the
            <emphasis>re-frame</emphasis> <literal>dispatch</literal>
            function to add the note to the app database. A
            <literal>path-&gt;note</literal> function is used to convert
            the stroke from the domain of euclidean space on the visual
            space of the canvas to the domain of time-pitch space for
            use with the audio synthesis system. The path-&gt;note
            function can be seen below:
          </para>
          <programlisting language="clojure">
(defn path-&gt;note [path first-point stage-size]
  &quot;Main entry point to this namespace&quot;
  (let [path-width (.. path -bounds -width)
        width      (:width stage-size)
        height     (:height stage-size)]
    {:freq        (domain/euclidean-&gt;freq (.. first-point -y) height)
     :onset       (domain/euclidean-&gt;time (.. first-point -x) width)
     :duration    (domain/euclidean-&gt;time path-width width)
     :velocity    0.5
     :enabled     true
     :probability 1.0
     :color       @(col/as-css (get colors (rand-int 100)))
     :height      (.. path -bounds -height)
     :width       (.. path -bounds -width)
     :envelopes   {:frequency {:raw     (paper-path-&gt;vec path [width height])
                               :sampled (paper-path-&gt;sample path stage-size)}
                   :vibrato   (reduce (fn [a b] (assoc a b [b 0])) (sorted-map) (range 11))}}))
</programlisting>
          <para>
            The domain of time-pitch is used to store the notes in
            memory and makes it possible to maintain a relative
            relationship between the screen size and the drawn notes.
          </para>
          <para>
            The dispatched note event is handled by a
            <emphasis>re-frame</emphasis>
            <literal>reg-event-db</literal> handler which describes the
            alteration that is required to be made to the database. It
            also uses a series of interceptors, to perform validation of
            the database and to remove some of the boilerplate from the
            event handler functions. Interceptors are similar
            conceptually to middleware and is the place where all of the
            side effects arising from an event are actioned. Moving
            application side effects to this placed ensures that they
            are isolated and means that the majority of the program can
            be kept as pure logic improving to make it easier to test,
            debug and reason about. As can be seen the handler function
            is very simple:
          </para>
          <programlisting language="clojure">
(reg-event-db
 :note-add
 note-interceptors
 (fn [notes [note-info]]
   (let [id    (allocate-next-id notes)
         note  (assoc note-info :id id)]
     (if (&gt;= (:duration note) 0.001)
       (assoc notes id note)
       notes))))
</programlisting>
          <para>
            This does a simple check to make sure that note has a
            minimum duration and if so alters the notes vector to
            include the new vector which will instruct
            <emphasis>re-frame</emphasis> to update it's internal atom
            with this new state.
          </para>
          <para>
            The structure of the note hashmap is defined using
            <emphasis>clojure.spec</emphasis>, a core library to perform
            data validation and a tool that may be used similarly to
            types in strongly typed languages. The note specs are
            defined as follows:
          </para>
          <programlisting language="clojure">
(s/def ::id int?)
(s/def ::freq float?)
(s/def ::onset float?)
(s/def ::duration float?)
(s/def ::velocity float?)
(s/def ::color string?)
(s/def ::note (s/keys :req-un [::id ::freq ::onset ::duration]
                      :opt-un [::velocity]))
</programlisting>
          <para>
            Although note specified here, notes also have an
            <literal>envelopes</literal> key that stores frequency and
            vibrato envelopes. For example:
          </para>
          <programlisting language="clojure">
{:frequency {:raw [{:point [-0.01 99.7] :handle-in [0 100] :handle-out [0 99.8]}  ...
                   {:point [2.8 98.02] :handle-in [-0.10 100.4], :handle-out [0 100]}]
             :sampled [-0.36991368680641185 ... -2.172026174596564]}
 :vibrato {0 [0 0], ... 10 [10 0]}}
</programlisting>
          <para>
            The update in state spins <emphasis>re-frame</emphasis>'s
            subscription system into action and any views that are
            subscribed to the changed application state are now
            re-rendered. The <literal>graphics-notes</literal> view for
            instance is subscribed to <literal>:notes</literal>,
            <literal>:graphics-stage-width</literal>,
            <literal>:graphics-stage-height:</literal> and
            <literal>:mode</literal>:
          </para>
          <programlisting language="clojure">
(defn graphics-notes []
  (let [notes         (subscribe [:notes])
        width         (subscribe [:graphics-stage-width])
        height        (subscribe [:graphics-stage-height])
        ....]
    ;; playback-time @(subscribe [:playback-time])
    (into [:Group]
          (map (fn [note]
                 ^{:key (:id note)} [graphics-note* ...]) @notes))))
</programlisting>
          <para>
            When a note is added this render function will run which
            will call the <literal>[graphics-note*]</literal> component
            for each of the notes, and update the visual display of the
            screen to show the new note. A similar process happens in
            the audio system except, in this case, new web audio nodes
            are created, timeline events are queued up at the correct
            time and audio envelopes are setup that traces the curve of
            the drawn lines. (Adenot, 2017)
          </para>
          <programlisting language="clojure">
(defn fm-synth [{:keys [out] :as props} &amp; children]
  &quot;FM synth&quot;
  ;; The new synth is instanciated using js interop
  (let [synth (js/Tone.FMSynth. (clj-&gt;js (dissoc props :out)))]
    (reagent/create-class
     {:component-did-mount
      (fn []
        ;; The synth is connected to it's output (which is passed
        ;; in as a property to the component)
        (.. synth (connect out)))
      :reagent-render
      (fn [props &amp; children]
        ;; The render function renders a dummy span dom element and
        ;; renders it's children and passing it's synth as the out
        ;; for these components.
        (into [:span]
              (map (fn [child]
                     (assoc-in child [1 :out] synth))
                   children)))
      :component-will-unmount
      (fn []
        ;; Here we dispose of the synth
        ;; This will happen when the parent note is removed or when
        ;; a live code reload happens
        (.. synth dispose))})))
</programlisting>
          <para>
            The above shows the <literal>fm-synth</literal> which sits
            at the heart of the audio generating part of the system.
            Potential parent components of this would be audio effects
            or the master bus. It's child components are comprised of
            note events and envelopes that drive frequency changes over
            the course of the note playback. The composition of events,
            envelopes, synths, effects and channels is shown in
            truncated form:
          </para>
          <programlisting language="clojure">
;; Parent component is the project and
;; has settings such as tempo
[project {:project :settings}
 [master-bus {}
  ;; Master volume is set here
  [volume {:volume :settings}
   ;; Adds a simple reverb effect
   [reverb-effect {}
    [
     ;; First note
     ;; Each note has a vibrato effect
     [vibrato-effect {}
      ;; Envelope to control vibrato depth
      [timeline-evt evt
       [envelope {:param &quot;depth&quot;
                  :env   vib-env}]]
      ;; The fm synth that generates the signal
      [fm-synth (get-in evt [:preset])
       ;; Timeline event that takes care of queuing
       ;; it's child components
       [timeline-evt evt
        ;; In this case a note
        [note {:note :settings}]
        ;; And a frequency envelope
        [envelope {:param &quot;frequency&quot;
                   :state state
                   :env   freq-env}]]]]
     ;; Second note
     [vibrato-effect {}
      ;; ...
      ]
     ;; ...
     ]]]]]
</programlisting>
        </listitem>
        <listitem>
          <para>
            Editing notes
          </para>
          <para>
            Once created, a number of editing operations can be
            performed on the notes. Apart from the
            <emphasis role="strong">delete</emphasis> action all of
            these operations involve first selecting the note on the
            <emphasis role="strong">mouse down</emphasis> event,
            carrying out the editing of the note and completing the
            action with the mouse up event. The particular tool can be
            activated by clicking the icon the right-hand side of the
            screen (see ). Clicking one of these dispatches the
            <literal>:change-mode</literal> event and changes the
            <emphasis role="strong">app-db</emphasis> to the specified
            mode as well as setting the application to state
            <literal>:pending</literal>. Flowing from this change are a
            number of changes:
          </para>
          <orderedlist spacing="compact">
            <listitem>
              <para>
                The relevant paper.js tool is activated which will route
                further mouse events to the appropriate dispatch
                handlers. For instance, when the
                <literal>:resize</literal> mode is selected,
                <emphasis role="strong">mouse move</emphasis> events
                will be handled by
                <literal>:resize-tool-pointer-move</literal>.
              </para>
            </listitem>
            <listitem>
              <para>
                It updates the visual look of the cursor to remind the
                user which of mode they are in.
              </para>
            </listitem>
            <listitem>
              <para>
                A visual indicator is shown around the icon of the
                selected mode
              </para>
            </listitem>
          </orderedlist>
          <para>
            The overall effect of this is an activation of a number of
            different modes, each of which will now be discussed.
          </para>
          <para>
            In the case of deleting notes, the event is simply raised in
            the <emphasis role="strong">click</emphasis> event of the
            note, which takes care of dispatching the
            <literal>:note-delete</literal> event.
          </para>
          <para>
            This is handled by a very simple handler,
            <literal>(dissoc notes note-id)</literal> that instructs
            <emphasis>re-frame</emphasis> to remove the note in question
            from the database. Similar to the process that occurs when a
            note is added, the visual display is now updated to no
            longer show the note. In addition, the signal generator,
            effects, envelopes and events associated with the note are
            removed from the audio component tree with
            <emphasis>React.js</emphasis>'s lifecycle events taking care
            to clean up any synth's and effects by calling their
            <literal>dispose</literal> methods.
          </para>
          <para>
            Moving notes and resizing notes work very similarly and both
            follow the most basic pattern described above of selection,
            manipulation, and completion. The note selection event is
            raised from the note's <emphasis role="strong">mouse
            down</emphasis> event handler, dispatching the
            <literal>:note-select</literal> event with the
            <emphasis role="strong">note-id</emphasis>. This updates the
            <emphasis role="strong">app-db</emphasis> to set the
            <literal>:active-note-id: to the received id, sets the app state to
            =:active</literal>, sets the note state to be
            <literal>:active</literal> and resets previously active
            notes to state, <literal>:normal</literal>. This state
            update changes the visual display of the active note to be
            highlighted by slightly lightening the colour of the note's
            surrounding glow. More importantly, the note now becomes the
            active item on which further user interactions are performed
            on. In the case of the
            <emphasis role="strong">move</emphasis> tool the stored
            onset and pitch properties of the note are altered to
            reflect the position of the user's mouse. This, in turn,
            updates the visual display of the note and causes the audio
            system to re-queue the note events to the new onset and
            pitch values. The <emphasis role="strong">resize</emphasis>
            tool, on the other hand, changes the size of the starting
            node of the note and alters the velocity accordingly. The
            altered size is calculated from the coordinates of the
            <emphasis role="strong">mouse down</emphasis> event and is
            clamped to a maximum size that corresponds to the maximum
            velocity of the note.
          </para>
          <para>
            The slightly more esoteric
            <emphasis role="strong">probability</emphasis> tool works in
            a very similar fashion to the
            <emphasis role="strong">resize</emphasis> tools and again
            creates adjustments that work relative to
            <emphasis role="strong">mouse down</emphasis> coordinates.
            The visual effect that this creates, however, is a dulling
            of the saturation of the note. Its effect is to add an
            element of randomness and depending on how saturated the
            note is, it will cause the note to randomly skip. The code
            for this is below: #=INCLUDE:
            &quot;ch5-code.org::probability-queueing&quot; If the
            probability is set to <literal>1.0</literal> (which is the
            default) or fully saturated the note will play on every
            loop. If however, it is below this it will skip in a random
            but probabilistic fashion, adding a small amount of
            stochasticism to playback.
          </para>
          <para>
            The vibrato tool is the most complex in its implementation
            and involves a small popup UI element that allows the user
            to draw in a vibrato envelope which is visually reflected in
            the note lines as an overlaid sinewave. After selection
            occurs, in the usual way, <emphasis role="strong">mouse
            move</emphasis> events dispatch to the
            <literal>:vibrato-continue</literal> handler which updates a
            vibrato modulation parameter in the note, a single float
            value that represents the current real-time vibrato value.
            This modulation parameter is passed back to the view through
            properties where triggers a
            <literal>:component-did-update</literal> function call. It
            is here that a dispatch is made to the
            <literal>:vibrato-envelope-point</literal> to create the
            envelope point. The reason for the back and forth between
            the view and the event handlers is to allow the event
            handlers to manage the state changes but deal with the
            specifics of the geometry of the vibrato overlay in the
            view. It is probably best illustrated in the provided figure
            ().
          </para>
          <para>
            A visual overlay is shown to the user when a vibrato action
            is started, that shows a 10 point envelope, whose points are
            all set to <literal>0.0</literal> by default. The user can
            then drag at varying heights at various horizontal points on
            the axis and create the time-varying vibrato envelope. This
            envelope is visualised (and remains visible after the
            vibrato operation has completed) with a sinewave that tracks
            the curve of the frequency envelope and varies its amplitude
            depending on the strength of the vibrato at that particular
            point. The code to achieve this is below:
          </para>
        </listitem>
      </orderedlist>
    </section>
    <section xml:id="secondary-functionality">
      <title>Secondary functionality</title>
      <para>
        Aside from the core functionality of note creation and editing,
        a number of other use cases had to be catered for to make the
        application useable. This included transport controls, to allow
        the user to start and stop playback; some simple animation to
        show which notes are being played and to tighten the link
        between the visuals and the audio; undo and redo functionality;
        a fullscreen toggle to maximise the immersiveness; as well as
        the ability to save and load sketches. While not central to the
        concept, standard usability features such as undo/redo would be
        a noticeable omission that would decrease beginner friendliness.
        Some of these extra features were made more straightforward than
        would normally be the case due to the architecture of placing
        the state in a single place, the <emphasis>re-frame</emphasis>
        <emphasis role="strong">app db</emphasis>.
      </para>
      <para>
        Both the transport and the fullscreen mechanism consisted of a
        simple finite state machine (FSM) that who's transitions are
        caused by user actions. The transport state machine responds to
        both the button click and
        <emphasis role="strong">spacebar</emphasis> keypresses to toggle
        its state between playing and stopped and based on this,
        triggering playback in <emphasis>tone.js</emphasis>. The
        fullscreen FSM transitions occur not only when the user clicks
        the fullscreen button but also when the browser fullscreen
        status changes. This keeps the UI in a correct and consistent
        state at all times, regardless of how the state is reached.
      </para>
      <para>
        Undo/redo and save/load functionality was relatively
        straightforward to implement for the reasons mentioned above
        (centrally located state) as well as the ease of serialising
        <emphasis>ClojureScript</emphasis> data. Adding undo/redo was as
        easy as adding an additional dependency and adding it as an
        interceptor to any events that needed to be undoable such as
        adding or deleting notes.
      </para>
      <para>
        The particular parts of the app state that needed to be restored
        were also configured and included the <literal>notes</literal>
        collection and the <literal>tempo</literal>. Both of these
        elements were saved and restored by the save/restore
        functionality which worked by serialising this data as a string
        and restoring with a single function call to
        <emphasis>ClojureScript</emphasis>'s
        <literal>reader/read-string</literal>.
      </para>
      <para>
        As outlined in the previous chapter,
        <emphasis>SonicSketch</emphasis> an influence of the design of
        the user experience is the <emphasis>Music Animation
        Machine</emphasis> which visualises music so that even
        nonmusicians can relate the visuals to the music unlike
        following a CMN score. To achieve this, the
        <literal>:note</literal> subscription is itself subscribed to
        <literal>:playback-beat</literal> a reactive value that is kept
        up to date via the use of events that fire on every animation
        frame. The subscription checks if the playback head is within
        the notes range and if so sets its <literal>:playing</literal>
        property to <literal>true</literal> and updates
        <literal>:playback-time</literal> to reflect the position of the
        playback head in the note. Similar to any other changes in
        state, the note views react to this and update their visual look
        accordingly which due to the quickly changing nature of the
        value leads to an animation.
      </para>
    </section>
  </section>
  <section xml:id="conclusion-2">
    <title>Conclusion</title>
    <para>
      This chapter outlined the development process that took place to
      bring SonicSketch to life. The early prototype work was detailed,
      including the porting of SonicPainter to the web, all work that
      contributed conceptually to the final artifact. The setup of the
      technical architecture to enable a live code reloading workflow
      was outlined. An in depth description of the primary functionality
      that enables users to draw and edit notes on the screen was given,
      followed by a more brief look at secondary functionality such as
      undo, saving and note animation.
    </para>
  </section>
</section>
<section xml:id="evaluation">
  <title>Evaluation</title>
  <section xml:id="introduction-6">
    <title>Introduction</title>
    <para>
    </para>
  </section>
  <section xml:id="initial-pilot-test">
    <title>Initial pilot test</title>
    <para>
    </para>
  </section>
  <section xml:id="exhibition">
    <title>Exhibition</title>
    <para>
      As you can see in the figure , the majority of activity is in the
      upper regions of the frequency spectrum and concentrated at the
      beginning and the middle of the screen.
    </para>
    <para>
    </para>
  </section>
  <section xml:id="performance-issues">
    <title>Performance issues</title>
    <para>
    </para>
  </section>
  <section xml:id="conclusion-3">
    <title>Conclusion</title>
    <para>
    </para>
  </section>
</section>
<section xml:id="conclusion-and-further-work">
  <title>Conclusion and further work</title>
  <section xml:id="summary-of-work-completed">
    <title>Summary of work completed</title>
    <para>
    </para>
  </section>
  <section xml:id="broader-implications-of-development">
    <title>Broader implications of development</title>
    <para>
    </para>
  </section>
  <section xml:id="future-work">
    <title>Future work</title>
    <section xml:id="performance-improvements">
      <title>Performance improvements</title>
      <para>
      </para>
    </section>
    <section xml:id="broaden-visual-language">
      <title>Broaden visual language</title>
      <para>
      </para>
    </section>
    <section xml:id="allow-for-larger-structures">
      <title>Allow for larger structures</title>
      <para>
      </para>
    </section>
    <section xml:id="d-spaces-vr-spatial-audio">
      <title>3D spaces, VR, spatial audio</title>
      <para>
      </para>
    </section>
  </section>
</section>
<section xml:id="bibliography">
  <title>Bibliography</title>
  <anchor id="refs" />
  <para id="ref-ableton_live_2017">
    Ableton, L. (2017) <emphasis>Live Set Export Documentation |
    Ableton</emphasis>. Available at:
    <link xlink:href="https://ableton.github.io/export/">https://ableton.github.io/export/</link>
    (Accessed: 17 August 2017).
  </para>
  <para id="ref-adenot_web_2017">
    Adenot, P. (2017) <emphasis>Web Audio API performance and debugging
    notes</emphasis>. Available at:
    <link xlink:href="https://padenot.github.io/web-audio-perf/">https://padenot.github.io/web-audio-perf/</link>
    (Accessed: 4 August 2017).
  </para>
  <para id="ref-bell_journal_2015">
    Bell, A., Hein, E. and Ratcliffe, J. (2015) <emphasis>Journal on the
    Art of Record Production ¬ª Beyond Skeuomorphism: The Evolution of
    Music Production Software User Interface Metaphors</emphasis>.
    Available at:
    <link xlink:href="http://arpjournal.com/beyond-skeuomorphism-the-evolution-of-music-production-software-user-interface-metaphors-2/">http://arpjournal.com/beyond-skeuomorphism-the-evolution-of-music-production-software-user-interface-metaphors-2/</link>
    (Accessed: 23 April 2017).
  </para>
  <para id="ref-caramiaux_adaptive_2015">
    Caramiaux, B. <emphasis>et al.</emphasis> (2015) ‚ÄòAdaptive gesture
    recognition with variation estimation for interactive systems‚Äô,
    <emphasis>ACM Transactions on Interactive Intelligent Systems
    (TiiS)</emphasis>, 4(4), p. 18. Available at:
    <link xlink:href="http://dl.acm.org/citation.cfm?id=2643204">http://dl.acm.org/citation.cfm?id=2643204</link>
    (Accessed: 21 April 2017).
  </para>
  <para id="ref-duignan_abstraction_2010">
    Duignan, M., Noble, J. and Biddle, R. (2010) ‚ÄòAbstraction and
    activity in computer-mediated music production‚Äô, <emphasis>Computer
    Music Journal</emphasis>, 34(4), pp. 22‚Äì33. Available at:
    <link xlink:href="http://www.mitpressjournals.org/doi/pdf/10.1162/COMJ_a_00023">http://www.mitpressjournals.org/doi/pdf/10.1162/COMJ_a_00023</link>
    (Accessed: 17 August 2017).
  </para>
  <para id="ref-levin_painterly_2000">
    Levin, G. (2000) <emphasis>Painterly interfaces for audiovisual
    performance</emphasis>. Massachusetts Institute of Technology.
    Available at:
    <link xlink:href="https://dspace.mit.edu/handle/1721.1/61848">https://dspace.mit.edu/handle/1721.1/61848</link>
    (Accessed: 23 April 2017).
  </para>
  <para id="ref-mann_interactive_2015">
    Mann, Y. (2015) ‚ÄòInteractive music with tone. js‚Äô, in
    <emphasis>Proceedings of the 1st annual Web Audio
    Conference</emphasis>. Available at:
    <link xlink:href="http://wac.ircam.fr/pdf/wac15_submission_40.pdf">http://wac.ircam.fr/pdf/wac15_submission_40.pdf</link>
    (Accessed: 28 July 2017).
  </para>
  <para id="ref-martin_clean_2012">
    Martin, R. (2012) <emphasis>The Clean Architecture | 8th
    Light</emphasis>. Available at:
    <link xlink:href="https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html">https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html</link>
    (Accessed: 15 August 2017).
  </para>
  <para id="ref-wigdor_brave_2011">
    Wigdor, D. and Wixon, D. (2011) <emphasis>Brave NUI world: Designing
    natural user interfaces for touch and gesture</emphasis>.
    Burlington, Mass: Morgan Kaufmann.
  </para>
</section>
</article>
